"""
ğŸ’¾ Backup Manager - Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
"""

import asyncio
import json
import logging
import os
import shutil
import zipfile
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any

logger = logging.getLogger(__name__)

class BackupManager:
    """Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ"""
    
    def __init__(self, db_handler, backup_dir: str = "backups"):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ"""
        self.db = db_handler
        self.backup_dir = Path(backup_dir)
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
        self.max_backups = 30  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù†Ø³Ø® Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
        self.auto_backup_interval = 24  # Ø³Ø§Ø¹Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù†Ø³Ø® Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
        self.last_backup_time = None
        
        logger.info(f"ğŸ’¾ Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù…Ù‡ÙŠØ£: {self.backup_dir}")
    
    async def create_backup(self, backup_name: str = None, include_data: bool = True) -> Optional[Path]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if backup_name:
                backup_name = f"{backup_name}_{timestamp}"
            else:
                backup_name = f"whatsapp_bot_backup_{timestamp}"
            
            backup_path = self.backup_dir / backup_name
            backup_path.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"ğŸ’¾ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {backup_name}")
            
            # 1. Ù†Ø³Ø® Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if include_data and self.db.is_sqlite:
                db_backup_path = backup_path / "database.db"
                await self.db.backup_database(str(db_backup_path))
            
            # 2. Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
            settings = await self._export_settings()
            settings_file = backup_path / "settings.json"
            with open(settings_file, 'w', encoding='utf-8') as f:
                json.dump(settings, f, ensure_ascii=False, indent=2)
            
            # 3. Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ÙˆØ¶Ø¹
            state = await self._export_state()
            state_file = backup_path / "state.json"
            with open(state_file, 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
            
            # 4. Ù†Ø³Ø® Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¬Ù„Ø³Ø§Øª
            sessions_dir = Path("sessions")
            if sessions_dir.exists():
                sessions_backup_dir = backup_path / "sessions"
                shutil.copytree(sessions_dir, sessions_backup_dir)
            
            # 5. Ù†Ø³Ø® Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            data_dir = Path("data")
            if data_dir.exists():
                data_backup_dir = backup_path / "data"
                shutil.copytree(data_dir, data_backup_dir, ignore=shutil.ignore_patterns('*.db'))
            
            # 6. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ÙˆØµÙ Ø§Ù„Ù†Ø³Ø®Ø©
            metadata = {
                'backup_name': backup_name,
                'created_at': datetime.now().isoformat(),
                'version': '1.0.0',
                'components': ['database', 'settings', 'state', 'sessions', 'data'],
                'total_size': self._get_directory_size(backup_path)
            }
            
            metadata_file = backup_path / "metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # 7. Ø¶ØºØ· Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
            zip_path = backup_path.parent / f"{backup_name}.zip"
            self._create_zip(backup_path, zip_path)
            
            # 8. Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø¤Ù‚Øª
            shutil.rmtree(backup_path)
            
            # ØªØ­Ø¯ÙŠØ« ÙˆÙ‚Øª Ø¢Ø®Ø± Ù†Ø³Ø®Ø©
            self.last_backup_time = datetime.now()
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            await self.cleanup_old_backups()
            
            logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {zip_path.name} ({self._bytes_to_human(zip_path.stat().st_size)})")
            
            return zip_path
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {e}")
            return None
    
    async def _export_settings(self) -> Dict[str, Any]:
        """ØªØµØ¯ÙŠØ± Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª"""
        try:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
            settings = {}
            
            # Ù‡Ø°Ù‡ Ø¯Ø§Ù„Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© - ØªØ­ØªØ§Ø¬ Ù„Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙØ¹Ù„ÙŠ
            # ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø³ØªÙ‚ÙˆÙ… Ø¨Ø¬Ù…Ø¹ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
            
            return {
                'exported_at': datetime.now().isoformat(),
                'settings': settings
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØµØ¯ÙŠØ± Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª: {e}")
            return {}
    
    async def _export_state(self) -> Dict[str, Any]:
        """ØªØµØ¯ÙŠØ± Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…"""
        try:
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            state = {
                'exported_at': datetime.now().isoformat(),
                'system_state': {
                    'is_running': True,  # ÙŠØ¬Ø¨ Ø§Ù„Ø­ØµÙˆÙ„ Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù…
                    'connected_sessions': 0,  # ÙŠØ¬Ø¨ Ø§Ù„Ø­ØµÙˆÙ„ Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù…
                    'active_tasks': []
                },
                'database_info': await self.db.get_database_info(),
                'statistics': await self.db.get_statistics(days=7)
            }
            
            return state
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØµØ¯ÙŠØ± Ø§Ù„Ø­Ø§Ù„Ø©: {e}")
            return {}
    
    async def restore_backup(self, backup_path: Path) -> bool:
        """Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"""
        try:
            if not backup_path.exists():
                logger.error(f"âŒ Ù…Ù„Ù Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {backup_path}")
                return False
            
            logger.info(f"ğŸ”„ Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {backup_path.name}")
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¤Ù‚Øª
            extract_dir = self.backup_dir / f"restore_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            extract_dir.mkdir(parents=True, exist_ok=True)
            
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø±Ø´ÙŠÙ
                with zipfile.ZipFile(backup_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_dir)
                
                # Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ø§Ù„ÙˆØµÙ
                metadata_file = extract_dir / "metadata.json"
                if not metadata_file.exists():
                    logger.error("âŒ Ù…Ù„Ù ÙˆØµÙ Ø§Ù„Ù†Ø³Ø®Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
                    return False
                
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                logger.info(f"ğŸ“‹ Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ù†Ø³Ø®Ø©: {metadata['backup_name']}")
                
                # Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
                for component in metadata.get('components', []):
                    component_path = extract_dir / component
                    
                    if component == 'database' and component_path.exists():
                        # Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                        db_files = list(component_path.glob("*.db"))
                        if db_files:
                            await self.db.restore_database(str(db_files[0]))
                    
                    elif component == 'sessions' and component_path.exists():
                        # Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø¬Ù„Ø³Ø§Øª
                        sessions_dir = Path("sessions")
                        if sessions_dir.exists():
                            shutil.rmtree(sessions_dir)
                        shutil.copytree(component_path, sessions_dir)
                    
                    elif component == 'data' and component_path.exists():
                        # Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                        data_dir = Path("data")
                        if data_dir.exists():
                            # Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù…Ø¹ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¹Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                            for item in data_dir.iterdir():
                                if item.is_file() and not item.name.endswith('.db'):
                                    item.unlink()
                                elif item.is_dir():
                                    shutil.rmtree(item)
                        
                        # Ù†Ø³Ø® Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
                        for item in component_path.iterdir():
                            if item.is_file():
                                shutil.copy2(item, data_dir / item.name)
                            elif item.is_dir():
                                shutil.copytree(item, data_dir / item.name)
                
                logger.info("âœ… ØªÙ… Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­")
                return True
                
            finally:
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø¤Ù‚Øª
                if extract_dir.exists():
                    shutil.rmtree(extract_dir)
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {e}")
            return False
    
    async def list_backups(self) -> List[Dict[str, Any]]:
        """Ø¹Ø±Ø¶ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"""
        backups = []
        
        try:
            for file_path in self.backup_dir.glob("*.zip"):
                try:
                    stat = file_path.stat()
                    
                    backup_info = {
                        'name': file_path.name,
                        'path': str(file_path),
                        'size': stat.st_size,
                        'size_human': self._bytes_to_human(stat.st_size),
                        'created_at': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                        'modified_at': datetime.fromtimestamp(stat.st_mtime).isoformat()
                    }
                    
                    # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø±Ø§Ø¡Ø© metadata Ù…Ù† Ø§Ù„Ø£Ø±Ø´ÙŠÙ
                    try:
                        with zipfile.ZipFile(file_path, 'r') as zip_ref:
                            if 'metadata.json' in zip_ref.namelist():
                                with zip_ref.open('metadata.json') as f:
                                    metadata = json.load(f)
                                    backup_info['metadata'] = metadata
                    except:
                        pass
                    
                    backups.append(backup_info)
                    
                except Exception as e:
                    logger.debug(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø³Ø®Ø© {file_path.name}: {e}")
            
            # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ (Ø£Ø­Ø¯Ø« Ø£ÙˆÙ„Ø§Ù‹)
            backups.sort(key=lambda x: x['modified_at'], reverse=True)
            
            return backups
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¹Ø±Ø¶ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {e}")
            return []
    
    async def cleanup_old_backups(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©"""
        try:
            backups = await self.list_backups()
            
            if len(backups) > self.max_backups:
                backups_to_delete = backups[self.max_backups:]
                
                for backup in backups_to_delete:
                    try:
                        Path(backup['path']).unlink()
                        logger.info(f"ğŸ§¹ ØªÙ… Ø­Ø°Ù Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù‚Ø¯ÙŠÙ…Ø©: {backup['name']}")
                    except Exception as e:
                        logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø­Ø°Ù Ø§Ù„Ù†Ø³Ø®Ø© {backup['name']}: {e}")
                
                logger.info(f"ğŸ§¹ ØªÙ… ØªÙ†Ø¸ÙŠÙ {len(backups_to_delete)} Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù‚Ø¯ÙŠÙ…Ø©")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©: {e}")
    
    async def schedule_auto_backup(self):
        """Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
        try:
            while True:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ Ø­Ø§Ù† ÙˆÙ‚Øª Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
                if self._should_create_auto_backup():
                    logger.info("ğŸ• Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ØªÙ„Ù‚Ø§Ø¦ÙŠØ©...")
                    await self.create_backup("auto_backup")
                
                # Ø§Ù†ØªØ¸Ø§Ø± Ø³Ø§Ø¹Ø© Ù‚Ø¨Ù„ Ø§Ù„ÙØ­Øµ Ø§Ù„ØªØ§Ù„ÙŠ
                await asyncio.sleep(3600)
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ: {e}")
    
    def _should_create_auto_backup(self) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ØªÙ„Ù‚Ø§Ø¦ÙŠØ©"""
        if self.last_backup_time is None:
            return True
        
        time_since_last = datetime.now() - self.last_backup_time
        return time_since_last.total_seconds() >= (self.auto_backup_interval * 3600)
    
    def _create_zip(self, source_dir: Path, zip_path: Path):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù…Ø¶ØºÙˆØ·"""
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(source_dir):
                for file in files:
                    file_path = Path(root) / file
                    arcname = file_path.relative_to(source_dir)
                    zipf.write(file_path, arcname)
    
    def _get_directory_size(self, directory: Path) -> int:
        """Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù…Ø¬Ù„Ø¯"""
        total_size = 0
        for file_path in directory.rglob('*'):
            if file_path.is_file():
                total_size += file_path.stat().st_size
        return total_size
    
    def _bytes_to_human(self, size_bytes: int) -> str:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨Ø§ÙŠØªØ§Øª Ø¥Ù„Ù‰ ØµÙŠØºØ© Ù…Ù‚Ø±ÙˆØ¡Ø©"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f} TB"
    
    async def verify_backup(self, backup_path: Path) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"""
        try:
            if not backup_path.exists():
                return False
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„Ù Ù‡Ùˆ Ø£Ø±Ø´ÙŠÙ ZIP ØµØ§Ù„Ø­
            if not zipfile.is_zipfile(backup_path):
                return False
            
            # ÙØªØ­ Ø§Ù„Ø£Ø±Ø´ÙŠÙ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            with zipfile.ZipFile(backup_path, 'r') as zip_ref:
                required_files = ['metadata.json', 'settings.json', 'state.json']
                
                for file in required_files:
                    if file not in zip_ref.namelist():
                        logger.warning(f"âš ï¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù†Ø³Ø®Ø©: {file}")
                        return False
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„Ø£Ø±Ø´ÙŠÙ
                file_size = backup_path.stat().st_size
                if file_size < 1024:  # Ø£Ù‚Ù„ Ù…Ù† 1KB
                    logger.warning("âš ï¸ Ø­Ø¬Ù… Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ØµØºÙŠØ± Ø¬Ø¯Ù‹Ø§")
                    return False
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†Ø³Ø®Ø©: {e}")
            return False
    
    async def get_backup_stats(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"""
        try:
            backups = await self.list_backups()
            
            total_size = sum(b['size'] for b in backups)
            oldest_backup = backups[-1]['created_at'] if backups else None
            newest_backup = backups[0]['created_at'] if backups else None
            
            return {
                'total_backups': len(backups),
                'total_size': total_size,
                'total_size_human': self._bytes_to_human(total_size),
                'oldest_backup': oldest_backup,
                'newest_backup': newest_backup,
                'max_backups': self.max_backups,
                'auto_backup_interval': self.auto_backup_interval,
                'last_backup_time': self.last_backup_time.isoformat() if self.last_backup_time else None
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø³Ø®: {e}")
            return {}
